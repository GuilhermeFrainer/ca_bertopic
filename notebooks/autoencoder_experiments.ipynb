{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "DATA_PATH = \"../data/processed/embeddings_batches/batch_*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.scan_parquet(DATA_PATH)\n",
    "lf.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = lf.drop([\"text\", \"state\", \"embedding\"]).collect()\n",
    "embeddings_df = lf.select(\"embedding\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def reshape_metadata(df: pl.DataFrame, new_range: tuple = (-1, 1)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts metadata DataFrame to numpy array.\n",
    "    Scales variables as needed.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=new_range)\n",
    "    return scaler.fit_transform(df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleFusionAE(nn.Module):\n",
    "    def __init__(self, text_dim: int, meta_dim: int, latent_dim=50):\n",
    "        super(SimpleFusionAE, self).__init__()\n",
    "\n",
    "        input_dim = text_dim + meta_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, text, metadata):\n",
    "        x = torch.cat((text, metadata), dim=1)\n",
    "\n",
    "        latent_representation = self.encoder(x)\n",
    "\n",
    "        reconstructed_x = self.decoder(latent_representation)\n",
    "\n",
    "        return reconstructed_x, latent_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_arr = reshape_metadata(metadata_df)\n",
    "embeddings_arr = embeddings_df[\"embedding\"].to_numpy()\n",
    "\n",
    "model = SimpleFusionAE(text_dim=embeddings_arr.shape[1], meta_dim=metadata_arr.shape[1]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "x_embeddings = torch.tensor(embeddings_arr, dtype=torch.float32).cuda()\n",
    "x_metadata = torch.tensor(metadata_arr, dtype=torch.float32).cuda()\n",
    "\n",
    "dataset = TensorDataset(x_embeddings, x_metadata)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20\n",
    "\n",
    "# Training the model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Training model\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_text, batch_meta in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstructed_x, latent_representation = model(batch_text, batch_meta)\n",
    "\n",
    "        target = torch.cat((batch_text, batch_meta), dim=1)\n",
    "        loss = criterion(reconstructed_x, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss / len(dataloader):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass fused embeddings to the CPU to be stored\n",
    "# Doing this gradually as to avoid OOM\n",
    "\n",
    "inference_dataset = TensorDataset(x_embeddings, x_metadata)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "fused_embeddings_list = []\n",
    "with torch.no_grad():\n",
    "    for batch_text, batch_meta in inference_loader:\n",
    "        _, batch_latent = model(batch_text, batch_meta)\n",
    "        batch_latent_cpu = batch_latent.cpu().numpy()\n",
    "        fused_embeddings_list.append(batch_latent_cpu)\n",
    "\n",
    "fused_embeddings = np.vstack(fused_embeddings_list)\n",
    "fused_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame({\"fused_embeddings\": fused_embeddings}).write_parquet(\"../data/processed/fused_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/fusion_autoencoder.weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vanilla_bertopic = BERTopic(\n",
    "    umap_model=UMAP(n_components=5, min_dist=0.0, metric=\"cosine\"),\n",
    "    hdbscan_model=HDBSCAN(min_cluster_size=15, prediction_data=True),\n",
    "    vectorizer_model=CountVectorizer(stop_words=\"english\"),\n",
    "    ctfidf_model=ClassTfidfTransformer()\n",
    ")\n",
    "\n",
    "modded_bertopic = BERTopic(\n",
    "    umap_model=UMAP(n_components=5, min_dist=0.0, metric=\"cosine\"),\n",
    "    hdbscan_model=HDBSCAN(min_cluster_size=15, prediction_data=True),\n",
    "    vectorizer_model=CountVectorizer(stop_words=\"english\"),\n",
    "    ctfidf_model=ClassTfidfTransformer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = lf.select(\"text\").collect().to_series().to_list()\n",
    "vanilla_embeddings = lf.select(\"embedding\").collect().to_series().to_numpy()\n",
    "encoder_embeddings = pl.read_parquet(\"../data/processed/fused_embeddings.parquet\").select(\"fused_embeddings\").to_series().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_topics, vanilla_probs = vanilla_bertopic.fit_transform(\n",
    "    docs, embeddings=vanilla_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_topics, modded_probs = modded_bertopic.fit_transform(\n",
    "    docs, embeddings=encoder_embeddings\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca-bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
